---
layout: post
title:  "Generative Adversarial Networks"
categories: [machine learning, supervised learning, probabilistic programming]
tags: [machinelearning]
---

<small>credit: most content in this post is a summary of the paper [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661), published at NeurIPS 2014, by Ian Goodfellow and colleagues at the Unviersity of Montreal.</small>

The paper introduces a new generative model composed of two models trained simultaneously:
- a generative model G that captures the data distribution; and
- a discriminative model D that estimates the probability that a sample came from the training data rather than G;

The training procedure for G is to maximize the probability of D making a mistake. This framework is the minimax 2-player game. The *adversarial* framework comes from the fact that the generative model faces the discriminative model, that learns wether a sample is from the model distribution or from the data distribution. Quoting the authors: *"The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles."*

### Mathematical setup

The generative model generates samples by passing a random noise $z$ through a multilayer perceptron. The discriminative model is also a multilayer perceptron. Because both models are connected deep neural networks, training is performed regularly via backpropagation. The setup of both networks is the following:

- The generator $$ G(z \mid \theta_g)$$ is a multilayer perceptron with parameters $$\theta_g$$. We use $$p_z(z)$$ to represent a prior on input noise variables. The goal of the generator is to learn the distribution $$p_g$$ over data $$x$$, in order to fake it "well enough" and trick the discriminator;
  - The generator $$G$$ implicitly defines a probability distribution $$p_g$$ as the distribution of the samples $$G(z)$$ obtained when $$z \sim p_z$$;
- The discriminator $$D(x \mid \theta_d)$$ takes the input $$x$$ and based on its DNN parameters $$\theta_d$$ it outputs a single scalar representing the probability that $$x$$ came from the data rather than $$p_g$$;

The overall structure of the generator and discriminator training can be pictured as:

<p align="center"><img width="65%" height="65%" src="/assets/Generative-Adversarial-Networks/GAN.png"/><br/>
<small>The GAN model (image credit: Benjamin Striner, lecture notes CMU 11-785)</small></p>

We train the discriminator $$D$$ to best guess the source of the data, i.e. maximize the probability to assign the correct label to the training example and samples from $$G$$. We train the generator $$G$$ simultaneously to minimize $$1-D((G(z)))$$. In practice, "$$D$$ and $$G$$ play the following two-player minimax game with value function $$(G,D)$$":

<p align="center"><img width="70%" height="70%" src="/assets/Generative-Adversarial-Networks/minimax_loss.png"/></p>

The loss function is a sum of two penalization terms:
- on the first term we optimise the discriminator, such that on the long run, we expect all real inputs (drawn from the data, ie $$x \sim p_{data}$$) to be as correct as possible;
- on the second term we optimise the discriminator and discriminator, such that on the long run, we expect all fake inputs (generated by the discriminator, ie $$z \sim p_z$$) to be also as correct as possible;  

Finally, notice there is a $$log$$ term added to both expected value terms. This is because the objective function is the product of multiple probabilities, in this case $$D(x) * (1-D(G(z)))$$. Adding the $$log$$ makes the optimisation return the same optima --- as $$log$$ is monotonic --- while making it faster and more numerical stable. Thus:

$$\mathbb{E} \left[ log(D(x) * (1-D(G(z)))) \right] = \mathbb{E} \left[ log(D(x)) \right] + \mathbb{E} \left[ log(1-D(G(z))) \right]$$;
 
In section 4 (theoretical results) the authors show that indeed, when given enough capacity to the discriminator and generator, it is possible to retrieve the data generating distribution. In practice, it shows that this minimax game has a global optimum for $$p_g = p_{data}$$, and therefore the loss function can be optimized.


### Challenge: different convergence speed in the optimization of Generator and Discriminator

Because the objective function is much simpler, the discriminator trains much faster than the discriminator. To overcome it, training is performed by alternating between $$k$$ steps of optimizing $$D$$ and one step of optimizing $$G$$. The underlying rationale is that "early in learning, when $$G$$ is poor, $$D$$ can reject samples with high confidence because they are clearly different from the training data. In this case,  $$log(1 − D(G(z)))$$ saturates.

### Challenge: saturation of discriminator loss

What does this saturation mean? In practice, minimizing that expression forces it to be approximated to zero. When the discriminator performs significantly better than the generator, the updates to the discriminator are either inaccurate, or disappear. A probably explanation behind is that the generator gets heavily penalized, which leads to saturation in the value post-activation function, and the eventual gradient vanishing.

The work around is the following: note that $$D(G(z))$$ is the discriminator's classification of the generator's image, and is bounded between 0 (fake) and 1 (real). So in $$V(D,G)$$, the authors replace the second term  $$\log(1 − D(G(z)))$$ by $$\log D(G(z))$$ and $$min_G max_D$$ by $$max_G max_D$$ so that the optimisation landscape "provides much stronger gradients early in training".

Update: after thinking about this topic for a while, I believe that in the original $$min_D max_G$$ formulation, $$D$$ and $$G$$ require very small parameter (DNN weights) values so that $$D(G(z))$$ approaches 1 such and $$1-D(G(z))$$ approaches 0. Very small gradient values yield two main issues:
- mathematically, the update of the parameters is very small, so the update of parameters in the loss landscape is very slow --- remember that typically the update of weight is $$ w = w + \gamma \nabla L$$ for a learning rate $$\gamma$$;
- computationally, arithmetic operations over small values lead to incorrect (or "always zero") results due to insufficient floating point precision in the processor (typically 16 or 32 bits); 
Thus, replacing that expression by $$max_D max_G D(G(z))$$ will allow gradients to be much higher, therefore allowing the optimisation of that term to be mathematically faster and computationally accurate.  

### Challenge: Mode collapse

This challenge was not part of the original publication, and it was not discovered until later. In a regular scenario, we want GANs to generate a range of outputs, or ideally, a new random valid output for every random input to the generator. Instead, it may happen that **sometimes we have a monotonous output, i.e. a range of very similar outputs**. This phenomenon is called Mode Collapse and is [detailled in this paper](https://arxiv.org/abs/2012.09673) as: "This event occurs when the model can only fit a few modes of the data distribution, while ignoring the majority of them". In this article they propose a workaround using second-order gradient information. However, this is a field of intense research and several solutions have been proposed, to name a few: a different loss function (Wasserstein loss), [Unrolled GANs](https://arxiv.org/pdf/1611.02163.pdf) that incorporates current and future discriminator's classification in the loss function (so that generator can't over optimize for a single discriminator), [Conditional GANs](https://arxiv.org/abs/1411.1784), [VQ-VAEs](https://arxiv.org/abs/2202.01987), etc...

### Training algorithm

This final training algorithm is the following:

<p align="center"><img width="70%" height="70%" src="/assets/Generative-Adversarial-Networks/GAN_algorithm.png"/></p>

