---
layout: post
title:  "Building GPT from scratch, in Python and C++"
categories: [machine learning, Transformer, GPT, C++]
tags: [machinelearning]
---


Recently... / I came across .../ Andrej Karpathy's...

To original [Transformer]({{ site.baseurl }}{% post_url 2020-05-28-AI-Supercomputing-2 %}), in paper *<a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>*, presented a tool for translation based on a novel attention mechanism. Similarly to previous translation methods (e.g. RNNs with LSTMs, GRUs), it had two main components: an encoder that learns the structure of the source language, and a decode that learns the translation to the new language. We soon realized that the Transformer had much more to offer and could be used in many datatypes (time-sequences, images, etc). And most importantly, that the components could be "stacked" as a sequence of ML blocks, to perform higher-level congnitive tasks.

An example of such model is [BERT]({{ site.baseurl }}{% post_url 2020-05-28-AI-Supercomputing-2 %}), a stack of Transformer encoders. Here we will look at a decoder-only architectures (*a la [GPT-2](https://en.wikipedia.org/wiki/GPT-2)*), used for generating long sentences from a given input. The fundamental difference between the encoder and the decoder is the *mask* in the underlying attention mechanism -- we'll cover it later.

<p align="center">
<br/>
<img width="100%" height="100%" src="/assets/GPT-lite/all_models.png"/><br/>
<br/><small><b>Left:</b> the original transformer structure with an endocer and a decoder block. <b>Middle:</b> a single-block decoder-only model architecture. <b>Right:</b> the multi-block GPT-like decoder-only architecture, detailed here. <b>Source:</b> copied and adapted from <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></small>
</p>

### Parameters

The basic imports and the hyperparameters we will use:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# set the random seed, for reproducibility
torch.manual_seed(42)

# device: where to execute computation
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# how often to do an evaluation step
eval_interval = 1000

# number of training iterations
max_iters = 5000

# optimizer's learning rate
learning_rate=3e-4

# minibatch size, how many inputs to 'pack' per iteration 
batch_size = 64

# block size ie max number of training sequence.
# E.g for input ABCD, we have 4 training examples: A->B, AB->C, ABC->C, ABCD->E
block_size = 256

# size of the embeddings
n_embd = 384

# number of attention heads in the Multi-Attention mechanism
n_head = 6

# number of heads. this is the $d_k$ in the paper formulation of Attn. Mech
head_size = 16

# depth of the network as number of decoder blocks.
# Each block contains a normalization, an attention and a feed forward unit
n_layer = 6

# dropout rate (variable p) for dropout units
dropout = 0.2
```

As input dataset, we will use the [tiny shakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) dataset:

```python
#download it from the repository and load input data
!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
with open("input.txt") as f:
    text = f.read()
```

### Tokenization

We need to create an embedding that maps input characters to a numerical representation of *tokens*. There are several possible *tokenizers*, the most simple being an embedding that maps characters to integers directly:

```python
# collect sorted list of input characters and create string-to-int (stoi) and int-to-string (itos) representations:
chars = sorted(list(set(text)))
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }

# define encode and decode functions that convert strings to arrays of tokens and vice-versa
encode = lambda x: torch.tensor([stoi[ch] for ch in x], dtype=torch.long) #encode text to integers
decode = lambda x: ''.join([itos[i] for i in x]) #decode integers to text
vocab_size = len(stoi)
``` 

The *vocabulary size* (`vocab_size`) is simply the number of distinct tokens. In this case, `65` characters:
```python

!$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
```

, where the first one is the `\n`.

<p align="center"><br/>
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_embedding.png"/><br/>
<br/><small>The embedding module in our model, emphasized in red.</small></p>


For better context, we can pack several characters into each token. I.e, instead of encoding a character per token, we can encode entire words or subwords. Few possibilities of pre-existent tokenizers are [google's sentencepiece](https://github.com/google/sentencepiece), the word or subword tokenizers by [huggingface](https://huggingface.co/docs/transformers/tokenizer_summary), or the  Byte-Pair-Encoding (BPE) `tiktoken` used by OpenAI's GPT:

```python
import tiktoken
enc = tiktoken.encoding_for_model("gpt-4")
print(enc.n_vocab)
print(enc.encode("Hello world"))
```

This would output vocabulary size of `100277` and `[9906, 1917]` and the encodind of `"Hello World"`.

### Positional Encoddings

The Attention mechanism returns a set of vectors. Only by adding positional encondings to each token we can introduce the notion of sequence. In the origianal paper they use a positional embedding based on the sin/cos frequency (see my previous transformers post for details). For simplicity, we can use instead the `Embedding` function in pytorch. 

```python
    token_embedding_table = nn.Embedding(vocab_size, n_embd)
    position_embedding_table = nn.Embedding(block_size, n_embd)
```

<p align="center"><br/>
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_pos_enc.png"/><br/>
<br/><small>The positional encoding module in our model, emphasized in red.</small></p>

### Batching and dimensionality

```python
def get_batch(source):
  """ get batch of size block_size from source """
  
  # generate `batch_size` random offsets on the data 
  ix = torch.randint(len(source)-block_size, (batch_size,) )
  # collect `batch_size` subsequences of length `block_size` from source, as data and target
  x = torch.stack([source[i:i+block_size] for i in ix])
  # target is just x shifted right (ie the next predicted word)
  y = torch.stack([source[i+1:i+1+block_size] for i in ix])
  return x.to(device), y.to(device)

#test get_batch
xb, yb = get_batch(train_data)
print("input:\n",xb)
print("target:\n",yb)
for b in range(batch_size): #for every batches
  for t in range(block_size): #for each sequence in block
    context = xb[b,:t+1]
    target = yb[b,t]
    print(f"for input {context.tolist()} target is {target.tolist()}")
```

Finally, we will split create the tokens for our input dataset and do a 90%-10% train/validation data split:

```python
data = encode(text)  #use any encoder here
n = int(0.9*len(data))
train_data, valid_data = data[:n], data[n:]
```

with full, train and validation datasets sized 1115394, 1003854 and 111540 characters.


### Masked Attention Heads

The point of the attention mask is to only present each row (token) with the current and previous tokens in the sentence. The easiest (fixed) approach to do so is to create a lower diagonal matrix that weights each visible element to the total count of visible elements per row:

Given a random input value  `x` (shapped batch x time x channels):
```python
B, T, C, = 4, 8, 2
x = torch.randn(B,T,C) #shape (B,T,C)
```

we can compute an *uniform* attention matrix as:
```python
#attention matrix (lower triangular), a mask used to only show previous items to predict next item
wei = torch.tril(torch.ones((T,T), dtype=torch.float32, device=device))
#normalize mask so that it sums to one. use keepdim to make broadcast operation work later
wei /= wei.sum(dim=1, keepdim=True)
print(wei)
``` 

that outputs:
```python
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],
        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],
        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])
```

The `wei` matrix indicates how much *attention* each token should give to itself and previous tokens, normalized to 1. In this case, it's uniform.
We can do a dot-product of the input `x` by the attention `wei` and see the output of the attention head for the given `x`:

```python
out = wei @ x   # dot product shape: (B,T,T) @ (B,T,C) = (B,T,C)
print(out.shape)

# output: torch.Size([4, 8, 2])
```
 
The original paper formulates attention as:

$$
MultiHead(Q, K, V ) = Concat(head_1, ..., head_h)W^O
$$

$$
\text{where} head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)
$$

$$
\text{where} Attention(Q,K,V) = softmax \left( \frac{QK^T}{\sqrt{d_k}} \right)
$$


<p align="center"><br/>
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_attention.png"/><br/>
<br/><small>The multi-head (Nx) attention module in our model, emphasized in red.</small></p>


Let's start with the $W^Q$, $W^K$ and $W^V$ matrices, computed as a simple projection (Linear layer):

```python
key   = nn.Linear(C, head_size, bias=False) 
query = nn.Linear(C, head_size, bias=False)
value = nn.Linear(C, head_size, bias=False)
```

We can now compute the $Atetntion(Q,K,V)$ as:

```python
x = torch.randn(B,T,C) #shape (B,T,C)
k = key(x) #shape (B,T, head_size)
q = query(x) #shape (B,T, head_size)
wei = q @ k.transpose(-2, -1) #shape (B,T, head_size) @ (B,head_size,T) = (B,T,T)
wei *= head_size**-0.5 #scale by sqrt(d_k) as per paper, so that variance of the wei is 1
```

We then compute the output of the attention matrix as:
```python
tril = torch.tril(torch.ones(T,T))
wei = wei.masked_fill(tril==0, float('-inf')) #tokens only "talk" to previous tokens
wei = F.softmax(wei, dim=-1) #equivalent to the normalization operation above (-inf in upper diagonal will be 0)
v = value(x) # shape (B,T, head_size)
out = wei @ v # shape (B,T,T) @ (B,T,C) --> (B,T,C)
```

Note that `out = wei @ x` is the same inner dot-product of the previous items, but this time the attention weights are not uniformely distributed but are instead learnt parameters. And this is the main property behind the self-attention solves: the rationale is that the lower diagonal (the non-zeros part) will be learnt and change over time, in practice telling how much important one token must give to each of the previous tokens, in order to predict the next token.
This is what changes from the previous approach, where attention weights were uniform across all previous tokens. Now it is learnt and can be different for each token. I.e., instead of the raw average of all tokens in the sequence, we aggregate a them by a "value of importance" for each token.

Also, without the $\sqrt{d_k}$ normalisation, we would have diffused values in `wei`, and it would approximate a one-hot vector. This normalization creates a more sparse `wei` vector.

This mechanism we coded is called *self-attention* because the $K$, $Q$ and $V$ all come from the same input x. But attention is more general. `x` can be provided by a source, and $K ,$Q$ and $V$ may come from a different sources. This is called *cross attention* because we use input from one token and attention from another token.

As final remarks, note that elements across batches are always independent, i.e. no cross-batch attention. And in many cases, e.g. a string representation of chemical compounds, or sentiment analysis, there's no attention mask (i.e. all tokens can attent to all tokens), or there's a custom mask that fits the use case (e.g. main upper and lower diagonals to allow tokens to see their closest neighbour only). And we dont have any cross atttention between the encoder and decoder.

The multi-head attention is simply a concatenation of individual heads' outputs. Thus, the attention logic can be implemented as:

```python
class Head(nn.Module):
  
  def __init__(self, head_size=16):
    super().__init__()
    self.key   = nn.Linear(n_embd, head_size, bias=False) 
    self.query = nn.Linear(n_embd, head_size, bias=False)
    self.value = nn.Linear(n_embd, head_size, bias=False)
    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
    self.dropout = nn.Dropout(dropout) #randomly prevents some tokens from communicating with each other
    
  def forward(self, x):
    B,T,C = x.shape
    k = self.key(x) #shape (B,T, head_size)
    q = self.query(x) #shape (B,T, head_size)
    v = self.value(x) #shape (B,T, head_size)
    
    #compute self-attention scores
    wei = q @ k.transpose(-2, -1) #shape (B,T, head_size) @ (B,head_size,T) --> (B,T,T)
    wei *= head_size**-0.5 #scale by sqrt(d_k) as per paper, so that variance of the wei is 1
    wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)
    wei = F.softmax(wei, dim=-1) # (B, T, T)
    wei = self.dropout(wei)
    
    #perform weighted aggregation of values
    v = self.value(x) #shape (B,T, head_size)
    out = wei @ v # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)
    return out


class MultiHeadAttention(nn.Module):
  """ Multi-head attention as described in the paper. Simply a collection of heads with concatenated outputs."""
  def __init__(self, num_heads, head_size):
    super().__init__()
    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
    self.proj  = nn.Linear(n_embd, n_embd)
    self.dropout = nn.Dropout(dropout)
    
  def forward(self, x):
    out = torch.cat([head(x) for head in self.heads], dim=-1)
    out = self.proj(out)
    out = self.dropout(out)
    return out
```

### Feed Forward Network


The Feed-forward network is a simply single-layer Deep Neural Network and is pretty straighforward to implement:

```python
class FeedForward(nn.Module):
  """ the feed forward network (FFN) in the paper"""
  def __init__(self, n_embd):
    super().__init__()
    # Note: in the paper (section 3.3) we have d_{model}=512 and d_{ff}=2048.
    # Therefore the inner layer is 4 times the size of the embedding layer
    self.net = nn.Sequential(
        nn.Linear(n_embd, n_embd*4),
        nn.ReLU(),
        nn.Linear(n_embd*4, n_embd),
        nn.Dropout(dropout)
      )
  def forward(self, x):
    return self.net(x)
```

<p align="center"><br/>
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_feedforward.png"/><br/>
<br/><small>The feed forward network in our model, emphasized in red.</small></p>

### The GPT Block

We'll call a GPT *block* the sequence of a multi-head attention and a feedforward. There is some subtle improvements we'd like to emphasize. Because network can become too deep (and hard to train) for a high number of sequential blocks, we added skip connections to each block. Also, in the original paper, the layer normalization operation is applied *after* the attention and the feed-forward network, but before the skip connection. In modern days, it is common to apply it in the *pre-norm formulation*, where normalization is applied before the attention and the FFN. That's also what we'll do in the following code: 

```python
class Block(nn.Module):
  """ Transformer block: comunication (attention) followed by computation (FFN) """
  def __init__(self, n_embd, n_head):
    # n_embd: embedding dimension
    # n_heads : the number of heads we'd like to use
    super().__init__()
    head_size = n_embd // n_head
    self.sa = MultiHeadAttention(n_head, head_size)
    self.ffwd = FeedForward(n_embd)
    self.ln1 = nn.LayerNorm(n_embd)
    self.ln2 = nn.LayerNorm(n_embd)
    
  def forward(self, x):
    x = x + self.sa(self.ln1(x))
    x = x + self.ffwd(self.ln2(x))
    return x
```

<p align="center"><br/>
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_blocks.png"/><br/>
<br/><small>The GPT block(s) in our model, emphasized in red.</small></p>


### Final Model:

```python
class MyModel(nn.Module):
  def __init__(self, num_head=4, n_layer=3):
    super().__init__()
    # vocabulary embedding and positional embedding
    # reminder: an embedding is simillar to a linear layer but it does a lookup instead of a matrix-vector multiplication
    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
    self.position_embedding_table = nn.Embedding(block_size, n_embd)
    
    #sequence of attention heads and feed forward layers
    self.blocks = nn.Sequential( *[Block(n_embd, n_head=4) for _ in range(n_layer)])

    #one layer normalization layer after transformer blocs and before linear layer that oututs the vocabulary
    self.ln = nn.LayerNorm(n_embd)
    self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)      

  def forward(self, idx, targets=None):
    """ call the model with idx and targets (training) or without targets (generation)"""
    #idx and targets are both of shape (B,T)
    B, T = idx.shape
    tok_emb = self.token_embedding_table(idx) #shape (B,T,C)
    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #shape (T,C)
    x = tok_emb + pos_emb #shape (B,T,C)
    #x = self.sa_head(x) #shape (B,T,C)
    #x = self.sa_heads(x) # (B,T,C)
    #x = self.ffwd(x) #shape (B,T,C)
    x = self.blocks(x)
    x = self.ln(x)
    logits = self.lm_head(x) #shape (B,T,V)
        
    if targets is None: #when calling generate()
      loss = None
    else:
      B, T, C  = logits.shape
      logits = logits.view(B*T, C) #shape (B*T,C)
      targets = targets.view(-1) #shape (B*T)
      loss = F.cross_entropy(logits, targets)
    return logits, loss
```


### Train loop

Now we can instantiate the model and copy it to the GPU:

```python
m  = MyModel().to(device)
```

and initialize the train loop (no magic here):

```python
# train the model
optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)
for steps in range(max_iters):
  xb, yb = get_batch(train_data)   #get a batch of training data
  logits, loss = m(xb, yb)   #forward pass
  loss.backward()   #backward pass
  optimizer.step()   #update parameters
  optimizer.zero_grad(set_to_none=True)  #sets to None instead of 0, to save memory

  #print progress
  if steps % 100 == 0: print(f"step {steps}, loss {loss.item():.2f}")
    
  @torch.no_grad()
  # eval loop: we dont do backprop on this data, to save storing all intermediatte variables
  def eval_loss():
    xb, yb = get_batch(valid_data)   #get a batch of validation data
    _, loss = m(xb, yb)
    print(f"step {steps}, eval loss {loss.item():.2f}")
    return loss
  
  if steps % eval_interval == 0: eval_loss().item()
      
```

Now test the model. Pass a single token (the `\n` character, encoded as `0`) to the model as initial character, and let it generate a sequence of 500 tokens: 

```python
def generate(self, idx, max_new_tokens):
  """ given a context idx, generate max_new_tokens tokens and append them to idx """
  for _ in range(max_new_tokens):
    idx_cond = idx[:, -block_size:] #we can never have any idx longer than block_size
    logits, _ = self(idx_cond) #call fwd without targets
    logits = logits[:, -1, :] # shape (B, C)
    #convert logits to probabilities
    probs = F.softmax(logits, dim=-1) # shape (B, C)
    #randomly sample the next tokens, 1 for each of the previous probability distributions
    #(one could take instead the argmax, but that would be deterministic and boring)
    idx_next = torch.multinomial(probs, num_samples=1) # shape (B, 1)
    #append next token ix to the solution sequence so far
    idx = torch.cat([idx, idx_next], dim=-1) # shape (B, T+1)
  return idx

#a 1x1 tensor with batch size 1 and sequence length 1 and starting value 0 (0 is the \n character)
idx = torch.zeros((1,1), dtype=torch.long, device=device)

# test the same generate() function, now with the trained model
print(decode(m.generate(idx, max_new_tokens=500).tolist()[0]))
```

and this will start blabbing some text:

```
IVINILCHIUKI noes hereseoke

PIV:
Ansto not 'dk thit, lighinglest in, the tole Himpams witecoond My me,
Cothe pill cthermandunes
The yould hankenou sonogher dovings age's eenat orshe:
And? Camer ou heithande, sonteas
Ans d th sonce;s ee e a
Bet n severe ay an kin nthely; wid, min. se garfitin d, s I at nd d tlineay hanoro f;'s ikeff t maleanta t t'san bus weleng, c,
Ther coressoonerves se ivenind s,
An then, e g
Anand nvienowhin, an somend tisefffospup thid meewgonevand s,
pe
Anony a onaneyou ynuplot fouthaies angen hr imoupe?

E CAsinoveatot toooven
A cevet heplig tsest were sis d whelen fen CLUny cer:
Aid: a d?
UFrrand age thak?

CEDUKINIG m.
Hak's; te!
```

### Code and how to get to a full-size GPT

All done, for the full implementaion of this code, see the file <a href="/assets/GPT-lite/gptlite.py"/>gptlite.py</a>.

As a final remark, here we built a tiny implementation of GPT. Better results can be achieved by increasing several of the parameters above, such as batch size, larger attention and more blocks. The paper [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) details the scale of the real implementation of GPT-2:

<p align="center"><br/>
<img width="80%" height="80%" src="/assets/GPT-lite/table_gpt_size.png"/><br/>
<br/><small>The GPT block(s) in our model, emphasized in red.</small></p>

If we try to re-run the above code with the following parameters increase:
```
TODO add here
```

we can immediately see the performance improvement in the model generated response:
```
TODO add here
```

### Fine-tuning

This is just the first task of the GPU training (pre-training). For a better task-specific response, the model needs the fine-tunning phase, usually using reinforcement learning. TODO
