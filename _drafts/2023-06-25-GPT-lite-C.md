---
layout: post
title:  "Building a GPT model from scratch, in C++, with XLA"
categories: [machine learning, Transformer, GPT, C++]
tags: [machinelearning]
---

### Parameters

The basic imports and the hyperparameters we will use for our GPT lite model is:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# set the random seed, for reproducibility
torch.manual_seed(42)

# device: where to execute computation
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# how often to do an evaluation step
eval_interval = 100

# number of training iterations
max_iters = 500

# optimizer's learning rate
learning_rate=1e-4

# minibatch size, how many inputs to 'pack' per iteration 
batch_size = 3

# block size is the maximum sequence length used as input.
# E.g. for block_size 4 and input ABCD, we have 4 training examples: A->B, AB->C, ABC->C, ABCD->E
block_size = 4

# size of the embeddings
n_embd = 16

# number of attention heads in the Multi-Attention mechanism (the Nx in the GPT decoder diagram)
n_head = 6

# dimensionality of the attention head. This is the $d_k$ in the paper formulation of Attn. Mechanism
head_size = 8

# depth of the network as number of decoder blocks.
# Each block contains a normalization, an attention and a feed forward unit
n_layer = 6

# dropout rate (variable p) for dropout units
dropout = 0.2
```

As input dataset, we will use the [tiny shakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) text. First download it from a console:
```
$ wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
```

and then load it in python:

```python
with open("input.txt") as f:
    text = f.read()
```

### Tokenization

We need to create an embedding that maps input characters to a numerical representation of *tokens*. There are several possible *tokenizers*, the most simple being an embedding that maps characters to integers directly:

```python
# collect sorted list of input characters and create 
# string-to-int (stoi) and int-to-string (itos) representations:
chars = sorted(list(set(text)))
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }

# define encode and decode functions that convert strings to arrays of tokens and vice-versa
encode = lambda x: torch.tensor([stoi[ch] for ch in x], dtype=torch.long) #encode text to integers
decode = lambda x: ''.join([itos[i] for i in x]) #decode integers to text
vocab_size = len(stoi)
``` 

The vocabulary size `vocab_size` is simply the number of distinct tokens. In this case, `65` characters:
```python

!$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
```

, where the first character is the line break character `\n`.


{: style="text-align:center; font-size: small;"}
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_embedding.png"/>

{: style="text-align:center; font-size: small;"}
The embedding module in our model, emphasized in red.


For better context and smaller embeddings, we can pack several characters into each token, instead of encoding a character per token. As an example, we can encode entire words or subwords. Few possibilities of pre-existent tokenizers are [google's sentencepiece](https://github.com/google/sentencepiece), the word or subword tokenizers by [huggingface](https://huggingface.co/docs/transformers/tokenizer_summary), or the  Byte-Pair-Encoding (BPE) `tiktoken` used by OpenAI's GPT:

```python
import tiktoken
enc = tiktoken.encoding_for_model("gpt-4")
print(enc.n_vocab)
print(enc.encode("Hello world").tolist())
```

This would output vocabulary size of `100277` and `[9906, 1917]` and the encoding of `"Hello World"` as tokens and on itr original string representation.

### Positional Encoddings

The Attention mechanism returns a set of vectors. Only by adding positional encondings to each token we can introduce the notion of sequence. In the original paper the authors use a positional embedding based on the $$sin$$ and $$cos$$ frequencies (see my previous transformer post for details). For simplicity, we can use instead the `Embedding` function in pytorch. 

```python
token_embedding_table = nn.Embedding(vocab_size, n_embd)    # from tokens to embedding
position_embedding_table = nn.Embedding(block_size, n_embd) # from position to embedding
```

{: style="text-align:center; font-size: small;"}
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_pos_enc.png"/>

{: style="text-align:center; font-size: small;"}
The positional encoding module in our model, emphasized in red.

### Batching and dimensionality

First, we will do a train-validation data split of 90% and 10%:

```python
data = encode(text)  #use any encoder here
n = int(0.9*len(data))
train_data, valid_data = data[:n], data[n:]
```

For this input: the full, train and validation datasets have `1115394`, `1003854` and `111540` elements, respectively.

Batching in the attention mechanism is tricky. The model needs to accept a batch of several inputs, and each input will have a maximum size of `block_size`. For this to be possible, for a given input, the dataset includes all sequencies from size `1` to size `block_size`. For each input sequence from position `0` to `t`, the respective output is given by the element in positon `t+1`. This logic is better detailed by to following code:    

```python
def get_batch(source):
  """ get batch of size block_size from source """
  
  # generate `batch_size` random offsets on the data 
  ix = torch.randint(len(source)-block_size, (batch_size,) )
  # collect `batch_size` subsequences of length `block_size` from source, as data and target
  x = torch.stack([source[i:i+block_size] for i in ix])
  # target is just x shifted right (ie the predicted token is the next in the sequence)
  y = torch.stack([source[i+1:i+1+block_size] for i in ix])
  return x.to(device), y.to(device)


# test get_batch()
xb, yb = get_batch(train_data)
print("input:\n",xb)
print("target:\n",yb)

for b in range(batch_size): #for every batches
  print(f"\n=== batch {b}:")
  for t in range(block_size): #for each sequence in block
    context = xb[b,:t+1]
    target = yb[b,t]
    print(f"for input {context.tolist()} target is {target.tolist()}")
```

this would output, for a `block_size=4` and `batch_size=3`:
```
input:
 tensor([[45, 57,  1, 59],
        [ 1, 40, 43,  1],
        [39, 52, 42,  1]])

target:
 tensor([[57,  1, 59, 54],
        [40, 43,  1, 56],
        [52, 42,  1, 52]])

=== batch 0:
for input [45] target is 57
for input [45, 57] target is 1
for input [45, 57, 1] target is 59
for input [45, 57, 1, 59] target is 54

=== batch 1:
for input [1] target is 40
for input [1, 40] target is 43
for input [1, 40, 43] target is 1
for input [1, 40, 43, 1] target is 56

=== batch 2:
for input [39] target is 52
for input [39, 52] target is 42
for input [39, 52, 42] target is 1
for input [39, 52, 42, 1] target is 52
```

### Masked Attention Heads

The point of the attention mask is to define which elements are "visible" to other elements in the attention matrix, and by how much. A common example is to use a lower-diagonal attention matrix, that presents to each row (token), the current and previous tokens in the sentence. Each value in the attention matrix holds the "importance" that the column token infers -- compared to all other columns of that row -- when the row token needs to take a decision. Thus, the easiest implementation of an attention matrix is to simply add uniform weights across each row, so that all tokens have the same weight/importance:

Given a random input value  `x` (shaped batch x time x channels):
```python
B, T, C, = 4, 8, 2
x = torch.randn(B,T,C) #shape (B,T,C)
```

Note, for simplicity, we'll use the following letters for the shapes of arrays:
- size `C` for the *embedding size* of each token, defined by the `n_embed` on top;
- size `B` for the *batch size* defined above by `batch_size`;
- size `T` for the *time* dimension, or input sequence length, defined by `block_size`.  

We can compute an *uniform* attention matrix as:
```python
#attention matrix (lower triangular), a mask used to only show previous items to predict next item
wei = torch.tril(torch.ones((T,T), dtype=torch.float32, device=device))
#normalize mask so that it sums to one. use keepdim to make broadcast operation work later
wei /= wei.sum(dim=1, keepdim=True)
``` 

or in an alternative notation (useful later):
```python
tril = torch.tril(torch.ones(T,T))
wei = wei.masked_fill(tril==0, float('-inf'))
wei = F.softmax(wei, dim=-1) #equivalent to the normalization above
```

where both notations output as `wei` the following:

```python
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],
        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],
        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])
```

Again, the `wei` matrix indicates how much *attention* each token should give to itself and previous tokens, normalized to 1. In this case, it's uniform.
We can do a dot-product of the input `x` by the attention `wei` and see the output of the attention head for the given `x`:

```python
out = wei @ x   # dot product shape: (B,T,T) @ (B,T,C) = (B,T,C) ie ([4, 8, 2])
```

Both `x` and `out` have dimensionlity `[B, T, C]` ie `[4, 8, 2]`.  
 
Now we move to the non-uniform attention matrix. The original paper formulates attention as:

$$
MultiHead(Q, K, V ) = Concat(head_1, ..., head_h)W^O
$$

$$
\text{where } head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)
$$

$$
\text{where } Attention(Q,K,V) = softmax \left( \frac{QK^T}{\sqrt{d_k}} \right) \, V
$$


{: style="text-align:center; font-size: small;"}
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_attention.png"/>

{: style="text-align:center; font-size: small;"}
The multi-head (Nx) attention module in our model, emphasized in red.


Let's start with the $$W^Q$$, $$W^K$$ and $$W^V$$ matrices, computed as a simple projection (*linear layer*):

```python
key   = nn.Linear(C, head_size, bias=False) 
query = nn.Linear(C, head_size, bias=False)
value = nn.Linear(C, head_size, bias=False)
```

We can now compute the $$Attention(Q,K,V)$$ as:

```python
k = key(x) #shape (B,T, head_size)
q = query(x) #shape (B,T, head_size)
wei = q @ k.transpose(-2, -1) #shape (B,T, head_size) @ (B,head_size,T) = (B,T,T)
wei *= head_size**-0.5 #scale by sqrt(d_k) as per paper, so that variance of the wei is 1
```

We then adapt the (alternative) notation of the uniform attention above, and compute the output of the non-uniform attention matrix as:
```python
tril = torch.tril(torch.ones(T,T))
wei = wei.masked_fill(tril==0, float('-inf')) #tokens only "talk" to previous tokens
wei = F.softmax(wei, dim=-1) #equivalent to the normalization above (-inf in upper diagonal will be 0)
v = value(x) # shape (B,T, head_size)
out = wei @ v # shape (B,T,T) @ (B,T,C) --> (B,T,C)
```

Note that `out = wei @ x` is the same inner dot-product of the previous items, but this time the attention weights are not uniform, they are learnt parameters and change per query and over time. And **this is the main property and the main problem that the self-attention solves: non-uniform attention weights per query**. This is different than the uniform attention matrix where weights were uniform across all previous tokens, i.e. aggregation was just a raw average of all tokens in the sequence. Here we aggregate them by a "value of importance" for each token.

Also, without the $$\sqrt{d_k}$$ normalisation, we would have diffused values in `wei`, and it would approximate a one-hot vector. This normalization creates a more sparse `wei` vector.

This mechanism we coded is called *self-attention* because the $$K$$, $$Q$$ and $$V$$ all come from the same input `x`. But attention is more general. `x` can be given by a data source, and $$K$$ ,$$Q$$ and $$V$$ may come from different sources -- this would be called *cross attention*.

As final remarks, note that elements across batches are always independent, i.e. no cross-batch attention. And in many cases, e.g. a string representation of chemical compounds, or sentiment analysis, there can be no attention mask (i.e. all tokens can attend to all tokens), or there's a custom mask that fits the use case (e.g. main upper and lower diagonals to allow tokens to see their closest neighbour only). And here, we also don't have any cross atttention between the encoder and decoder.

The decoder includes a multi-head attention, which is simply a concatenation of individual heads' outputs. The `Head` and `MultiHeadAttention` modules can then be implemented as:

```python
class Head(nn.Module):
  
  def __init__(self, head_size=16):
    super().__init__()
    self.key   = nn.Linear(n_embd, head_size, bias=False) 
    self.query = nn.Linear(n_embd, head_size, bias=False)
    self.value = nn.Linear(n_embd, head_size, bias=False)
    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
    #Dropout in Attn head randomly prevents some tokens from seeing others sometimes
    self.dropout = nn.Dropout(dropout)
    
  def forward(self, x):
    B,T,C = x.shape
    k = self.key(x) #shape (B,T, head_size)
    q = self.query(x) #shape (B,T, head_size)
    v = self.value(x) #shape (B,T, head_size)
    
    #compute self-attention scores
    wei = q @ k.transpose(-2, -1) #shape (B,T, head_size) @ (B,head_size,T) --> (B,T,T)
    wei *= head_size**-0.5 #scale by sqrt(d_k) as per paper, so that variance of the wei is 1
    wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B,T,T)
    wei = F.softmax(wei, dim=-1) # (B, T, T)
    wei = self.dropout(wei)
    
    #perform weighted aggregation of values
    v = self.value(x) #shape (B,T, head_size)
    out = wei @ v # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)
    return out


class MultiHeadAttention(nn.Module):
  """ Multi-head attention as described in the paper.
      Simply a collection of heads with concatenated outputs."""

  def __init__(self, num_heads, head_size):
    super().__init__()
    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
    self.proj  = nn.Linear(n_embd, n_embd)
    self.dropout = nn.Dropout(dropout)
    
  def forward(self, x):
    out = torch.cat([head(x) for head in self.heads], dim=-1)
    out = self.proj(out)
    out = self.dropout(out)
    return out
```

### Feed Forward Network


The Feed-forward network (FFN) in the decoder is simply a single-layer Deep Neural Network and is pretty straighforward to implement:

```python
class FeedForward(nn.Module):
  """ the feed forward network (FFN) in the paper"""

  def __init__(self, n_embd):
    super().__init__()
    # Note: in the paper (section 3.3) we have d_{model}=512 and d_{ff}=2048.
    # Therefore the inner layer is 4 times the size of the embedding layer
    self.net = nn.Sequential(
        nn.Linear(n_embd, n_embd*4),
        nn.ReLU(),
        nn.Linear(n_embd*4, n_embd),
        nn.Dropout(dropout)
      )

  def forward(self, x):
    return self.net(x)
```

{: style="text-align:center; font-size: small;"}
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_feedforward.png"/>

{: style="text-align:center; font-size: small;"}
The feed forward network in our model, emphasized in red.

### The GPT Block

We'll call GPT *block* the sequence of a multi-head attention and a feedforward module. There are some subtle improvements we'd like to emphasize. Because the network can become too deep (and hard to train) for a high number of sequential blocks, we added skip connections to each block. Also, in the original paper, the layer normalization operation is applied *after* the attention and the feed-forward network, but before the skip connection. In modern days, it is common to apply it in the *pre-norm formulation*, where normalization is applied before the attention and the FFN. That's also what we'll do in the following code: 

```python
class Block(nn.Module):
  """ Transformer block: comunication (attention) followed by computation (FFN) """

  def __init__(self, n_embd, n_head):
    # n_embd: embedding dimension
    # n_heads : the number of heads we'd like to use
    super().__init__()
    head_size = n_embd // n_head
    self.sa = MultiHeadAttention(n_head, head_size)
    self.ffwd = FeedForward(n_embd)
    self.ln1 = nn.LayerNorm(n_embd)
    self.ln2 = nn.LayerNorm(n_embd)
    
  def forward(self, x):
    x = x + self.sa(self.ln1(x))
    x = x + self.ffwd(self.ln2(x))
    return x
```

{: style="text-align:center; font-size: small;"}
<img width="19%" height="19%" src="/assets/GPT-lite/gpt_lite_blocks.png"/>

{: style="text-align:center; font-size: small;"}
The GPT block(s) in our model, emphasized in red.


### Final Model

Putting it all together:

```python
class MyModel(nn.Module):

  def __init__(self, num_head=4, n_layer=3):
    super().__init__()
    # vocabulary embedding and positional embedding
    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
    self.position_embedding_table = nn.Embedding(block_size, n_embd)
    
    #sequence of attention heads and feed forward layers
    self.blocks = nn.Sequential( *[Block(n_embd, n_head=4) for _ in range(n_layer)])

    # one layer normalization layer after transformer blocks, and
    # one layer normalization before linear layer that oututs the vocabulary
    self.ln = nn.LayerNorm(n_embd)
    self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)      


  def forward(self, idx, targets=None):
    """ call the model with idx and targets (training) or without targets (generation)"""
    #idx and targets are both of shape (B,T)
    B, T = idx.shape
    tok_emb = self.token_embedding_table(idx) #shape (B,T,C)
    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #shape (T,C)
    x = tok_emb + pos_emb #shape (B,T,C)
    x = self.blocks(x)
    x = self.ln(x)
    logits = self.lm_head(x) #shape (B,T,V)
        
    if targets is None: #when calling generate()
      loss = None
    else:
      B, T, C  = logits.shape
      logits = logits.view(B*T, C) #shape (B*T,C)
      targets = targets.view(-1) #shape (B*T)
      loss = F.cross_entropy(logits, targets)
    return logits, loss

  def generate(self, idx, max_new_tokens):
    """ given a context idx, generate max_new_tokens tokens and append them to idx """
    for _ in range(max_new_tokens):
      idx_cond = idx[:, -block_size:] #we can never have any idx longer than block_size
      logits, _ = self(idx_cond) #call fwd without targets
      logits = logits[:, -1, :] # shape (B, C)
      #convert logits to probabilities
      probs = F.softmax(logits, dim=-1) # shape (B, C)
      #randomly sample the next tokens, 1 for each of the previous probability distributions
      #(one could take instead the argmax, but that would be deterministic and boring)
      idx_next = torch.multinomial(probs, num_samples=1) # shape (B, 1)
      #append next token ix to the solution sequence so far
      idx = torch.cat([idx, idx_next], dim=-1) # shape (B, T+1)
    return idx  
```


### Train loop

Now we can instantiate the model and copy it to the compute device:

```python
m  = MyModel().to(device)
```

We then initialize the optimizer and perform the train loop:

```python
# train the model
optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)
for steps in range(max_iters):
  xb, yb = get_batch(train_data)   #get a batch of training data
  logits, loss = m(xb, yb)   #forward pass
  loss.backward()   #backward pass
  optimizer.step()   #update parameters
  optimizer.zero_grad(set_to_none=True)  #sets to None instead of 0, to save memory

  #print progress
  if steps % 100 == 0: print(f"step {steps}, loss {loss.item():.2f}")
    
  @torch.no_grad()
  # eval loop: no backprop on this data, to avoid storing all intermediatte variables
  def eval_loss():
    xb, yb = get_batch(valid_data)   #get a batch of validation data
    _, loss = m(xb, yb)
    print(f"step {steps}, eval loss {loss.item():.2f}")
    return loss
  
  if steps % eval_interval == 0: eval_loss().item()
      
```


### Size matters

We will now test the model. We pass a single token (the `\n` character, encoded as token `0`) to the model as initial character, and let it generate a sequence of 500 tokens: 

```python
#a 1x1 tensor with batch size 1 and sequence length 1 and starting value 0 (0 is the \n character)
idx = torch.zeros((1,1), dtype=torch.long, device=device)

# test the same generate() function, now with the trained model
print(decode(m.generate(idx, max_new_tokens=500).tolist()[0]))
```

and this will start blabbing some text, which looks a bit random for now:

```
IVINILCHIUKI noes hereseoke

PIV:
Ansto not 'dk thit, lighinglest in, the tole Himpams witecoond My me,
Cothe pill cthermandunes
The yould hankenou sonogher dovings age's eenat orshe:
And? Camer ou heithande, sonteas
Ans d th sonce;s ee e a
Bet n severe ay an kin nthely; wid, min. se garfitin d,
s I at nd d tlineay hanoro f;'s ikeff t maleanta t t'san bus weleng, c,
Ther coressoonerves se ivenind s,
An then, e g
Anand nvienowhin, an somend tisefffospup thid meewgonevand s,
pe
Anony a onaneyou ynuplot fouthaies angen hr imoupe?

E CAsinoveatot toooven
A cevet heplig tsest were sis d whelen fen CLUny cer:
Aid: a d?
UFrrand age thak?

CEDUKINIG m.
Hak's; te!
```

This is because for the sake of this tutorial, we used very small parameters for our GPT lite model. Better results can be achieved by increasing several of the parameters above, such as batch size, larger attention and more GPT blocks. As an example, if you try the following:

```python
eval_interval = 100
max_iters = 5000
learning_rate=3e-4
batch_size = 128
block_size = 256
n_embd = 300
head_size = 32
n_layer = 10
dropout = 0.2
```

You will get a significant performance improvement:

```
Pettience! if Tybalt ally kny to the waywards
That we renow't I have satic I fain.
Provotest the office Paul wains up the dudght.

And MOLEO:
Then me findy,
I do from the king him. How going in him.

TRANIO:
Good mind beherd at the world hold I pray
Toke you remect. If all my vernant youth I bear you
Dod forth.

DUKE VINCENTIO:
And ambibed, my lords I am not thence unnat;
Why, how said, too determoly fear is mercy,
But whether by my swalind
Tless and Rome: what guess the catter
Than thou not pro
```

All done! For the full python implementation of the code presented, download the source in  <a href="/assets/GPT-lite/gptlite_cpp.zip">gptlite_cpp.zip</a>.

### accelerating the model with XLA

basically it merges several nodes of the graph of partial derivatives of the fwd/backward pass. Gives a super high speedup (like 7x on LLMs on training),

### accelerating the model with DeepSpeed

This gives us a limit of speed-up for a fixed hardware and implementation:

Current large language models are Transformer-based and very similar to the one above e.g. GPT, BERT, Chinchilla, Gopher, Bloom, MT-NLG, PaLM, and LaMDA. So it should be relatively easy to write them, simply by replacing the correspoding module above. There are three further ways to improve the above model:
- improving individual componentes in the model, e.g. LLama uses bytepair encoding (BPE), pre-normalization via RMSNorm, SwiGLU activations, rotary positional embeddings, AdamW optimizer, cosine learning rate scheduler), flash attention, ghost attention,
- accelerated linear algebra (XLA), a technique that ....

### benchmark of python, compiled pyton, and XLA implementations

### improving and accelaring the model with different modules

This module is a LLM model from the early days. LLM scaling and efficiency are increasing fast and state of the art models now rely on simply replacing individual components of the GPT model improving. As as example, LLama uses bytepair encoding (BPE), pre-normalization via RMSNorm, SwiGLU activations, rotary positional embeddings, AdamW optimizer, cosine learning rate scheduler), flash attention, ghost attention, etc.

GPT, BERT, Chinchilla, Gopher, Bloom, MT-NLG, PaLM, and LaMDA

Changing this code base to match other implementations should be relatively easy.

Also, performance modelling plays a bit role here. Chinchilla...


### Final remarks: scaling, training vs inference, sparse transformers

In order to scale $$100x$$ over GPT-3, OpenAI **did not use dense transformers**, due to the prohibitive compute cost. To be precise, the priority for scaling models is inference (cost and latency). In practice, training is expensive once, and inference in cheap but is needed to run billion of times, exceeding the training time. This justifies using  sparse model architectures: only a subset of parameters is activated during inference.  In terms of high throughput or low latency, hardware's limitation on communication and memory bandwidths is a blocker when using very large dense networks. Therefore, the claim is that "OpenAI is achieving human reading speed, with A100s, with a model larger than 1 trillion parameters, and they are offering it broadly at a low price of only $0.06 per 1,000 tokens. That’s because it is sparse, IE not every parameter is used." See the paper [https://arxiv.org/abs/2101.03961](https://arxiv.org/abs/2101.03961) for the Switch Transformer with sparsity and Mixture of Experts.

On the other hand, most ML training/inference hardware is GPU/TPU based, which relies heavily on vectorization for accelerated algebraic computation, but does not favour dense networks as sparse memory accesses are not efficient. 

To learn more about scaling, ML systems, on-device vs massive model deployment, and cost  see:
- [GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure), 
- [On Device AI – Double-Edged Sword](https://www.semianalysis.com/p/on-device-ai-double-edged-sword),
- [The AI Brick Wall – A Practical Limit For Scaling Dense Transformer Models, and How GPT 4 Will Break Past It](https://www.semianalysis.com/p/the-ai-brick-wall-a-practical-limit),
- [https://www.semianalysis.com/p/google-ai-infrastructure-supremacy](https://www.semianalysis.com/p/google-ai-infrastructure-supremacy).

### Sate of the art LLM: GPT-4 architecture

(Note: this section is based on sources whose claims were not verified: [GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure) and [https://twitter.com/gordic_aleksa/status/1679460829073231878](Aleksa Gordić's tweet).

GPT-4 is over 10x the size of GPT-3 (175 B), with circa ~1.8 trillion parameters across 120 layers.  
- It is not a dense transformer like e.g. PaLM or GPT-3. For sparsity, it uses Mixture Of Experts, composed of 16 experts, each of approx. 111B parameters. 
  - Each forward pass (the output for a given input sequence) utilizes approx 280B parameters and 560 TFLOPs, in contrast with ~1.8 trillion parameters and ~3,700 TFLOP that would be required for its dense counterpart (the full model).
- It uses multi-query attention (MQA) instead of multi-head attention (MHA).
- It includes a separate vision encoder - different than the text encoder - with cross-attention. The architecture is similar to Google DeepMind's Flamingo. This adds additional parameters on top of the 1.8T of GPT-4. It is fine-tuned with another ~2 trillion tokens, after the text-only pre-training.
- The length of the context (span of attention head) was 8k for the pre-training phase. A sequence of length 32k is implemented from the fine-tuning of the 8k after the pre-training. This MosaicML's blog post explains how to achieve this: https://github.com/mosaicml/llm-foundry/blob/main/TUTORIAL.md)

**Training setup and cost**: GPU-4 was trained on a network of A100s GPUs utilizing 8-way tensor parallelism (?) via DeepSpeed Stage 1. Beyond that, they are using 15-way pipeline parallelism. Also apparently they used DeepSpeed ZeRo Stage 1 or block-level Fully Sharded Data Parallel (FSDP)
  - The dataset was a total of ~13T tokens, plus millions of rows of instruction fine-tuning data from ScaleAI & internal tools (?).
  - OpenAI’s training FLOPS for GPT-4 is ~2.15e25, on ~25,000 A100s for 90 to 100 days at about 32% to 36% MFU. This low utilization is mostly due to a large numbers of failures that requires checkpointing and restarts during training.
  - Assuming that the cloud compute cost was about $1 per A100 hour, the training cost for this run alone would be about $63 million. "Today, the pre-training could be done with ~8,192 H100 in ~55 days for $21.5 million at $2 per H100 hour"

**Inference:** 

--
 OpenAI might be using speculative decoding on GPT-4's inference. See this paper: https://arxiv.org/abs/2211.17192
- 
The inference runs on a cluster of 128 GPUs. There are multiple of these clusters in multiple datacenters in different locations (it'll be hard for Elizier to nuke these xD). 8-way tensor parallelism and 16-way pipeline parallelism.

Original thread: https://archive.is/2RQ8X (strictly speaking the original one has been removed).



